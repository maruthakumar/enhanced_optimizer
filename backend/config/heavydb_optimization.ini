[columnar_storage]
# Fragment size for optimal GPU memory usage (75M rows)
fragment_size = 75000000

# Maximum chunk size for GPU processing (32M for memory efficiency)
max_chunk_size = 32000000

# Compression algorithm for storage efficiency
compression = LZ4

# Dictionary encoding for strategy names and categorical data
dictionary_encoding = true

# Run-length encoding for date columns with repeated values
run_length_encoding = true

[partitioning]
# Partitioning strategy for 82 trading days
strategy = date_based

# Weekly partitions (7 days each for 82 total days)
partition_size_days = 7

# Enable partition pruning for faster queries
partition_pruning = true

# Enable parallel partition scanning
parallel_scan = true

[indexing]
# Create temporal index for time-series queries
temporal_index = true

# Create hash index for strategy lookups
strategy_hash_index = true

# Use bloom filters for correlation matrix existence checks
correlation_bloom_filter = true

# Maximum number of indexes per table to avoid performance penalty
max_indexes_per_table = 5

[data_encoding]
# Use DATE type for daily trading data (not TIMESTAMP)
date_type = DATE

# Use DOUBLE precision for P&L values
strategy_precision = DOUBLE

# Decimal scale for financial precision (6 decimal places)
strategy_scale = 6

# Optimize NULL value storage
null_optimization = true

[memory_optimization]
# GPU memory limit (8GB for production)
gpu_memory_limit_gb = 8

# CPU memory limit (16GB for fallback)
cpu_memory_limit_gb = 16

# Process strategies in batches of 1000 for memory efficiency
batch_size_strategies = 1000

# Enable lazy loading for large datasets
lazy_loading = true

[performance_targets]
# Production performance benchmarks from story requirements

# Load time for 39.2 MB file (< 30 seconds)
load_time_seconds = 30

# Memory usage during operations (< 2 GB)
memory_usage_gb = 2

# Correlation matrix query for top 1000 strategies (< 5 seconds)
correlation_query_seconds = 5

# Full portfolio optimization time (< 300 seconds)
optimization_seconds = 300

[correlation_optimization]
# Correlation calculation optimization parameters

# Chunk size for correlation calculation (strategies per chunk)
# Smaller values prevent timeout but increase total time
correlation_chunk_size = 50

# Maximum correlations per query to prevent timeout
# Lower values are safer for very large matrices
max_correlations_per_query = 500

# Timeout for correlation queries (seconds)
correlation_query_timeout = 300

# Use adaptive chunking based on matrix size
adaptive_chunking = true

# Chunk size for very large matrices (>1000 strategies)
large_matrix_chunk_size = 25

# Chunk size for huge matrices (>5000 strategies)
huge_matrix_chunk_size = 10

[production_specs]
# Production data specifications from story

# Input file size in MB
file_size_mb = 39.2

# Number of trading days in dataset
trading_days = 82

# Number of unique strategies
strategies = 25544

# Total data points (82 * 25,544)
data_points = 2094608

# Date range start
date_range_start = 2024-01-04

# Date range end
date_range_end = 2024-07-26

[validation]
# Validation settings for optimization testing

# Enable comprehensive validation
enable_validation = true

# Run benchmark tests after optimization
run_benchmarks = true

# Validate against performance targets
check_performance_targets = true

# Log detailed optimization information
detailed_logging = true

[gpu_optimization]
# GPU-specific optimization settings

# Enable GPU memory pool
enable_memory_pool = true

# GPU memory pool size (4GB)
memory_pool_size_gb = 4

# Enable GPU kernel fusion for correlation calculations
enable_kernel_fusion = true

# GPU batch size for wide table operations
gpu_batch_size = 10000

# Enable asynchronous GPU operations
async_gpu_ops = true

# Fallback strategy when GPU libraries not available
gpu_library_fallback = heavydb_sql

# Try to install GPU libraries if missing
auto_install_gpu_libs = false

# Use CPU correlation if GPU fails
cpu_correlation_fallback = true