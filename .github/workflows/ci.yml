name: Heavy Optimizer CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

env:
  PYTHON_VERSION: '3.10'
  CUDA_VERSION: '11.8'
  TEST_DATA_DIR: 'backend/tests/data'
  ANONYMIZED_SMALL_DATA: 'backend/tests/data/test_data_small.csv'
  ANONYMIZED_MEDIUM_DATA: 'backend/tests/data/test_data_medium.csv'
  ANONYMIZED_LARGE_DATA: 'backend/tests/data/test_data_large.csv'

jobs:
  lint:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install linting tools
        run: |
          python -m pip install --upgrade pip
          pip install black==23.3.0 flake8==6.0.0 mypy==1.3.0 isort==5.12.0
      
      - name: Run Black formatter check
        run: black --check backend/
      
      - name: Run isort import checker
        run: isort --check-only backend/
      
      - name: Run flake8 linter
        run: flake8 backend/ --max-line-length=100 --extend-ignore=E203,W503
      
      - name: Run mypy type checker
        run: mypy backend/ --ignore-missing-imports

  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: lint
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-xdist pytest-timeout
      
      - name: Prepare anonymized test data
        run: |
          # Check if test data exists, if not, create it
          if [ ! -f "${{ env.ANONYMIZED_SMALL_DATA }}" ]; then
            echo "Test data not found. Please run anonymization scripts locally and commit test data."
            echo "Note: Do not commit mapping files (*_mapping.json)"
            exit 1
          fi
      
      - name: Run unit tests for Parquet/Arrow/cuDF components
        run: |
          pytest backend/tests/unit/ -v --cov=backend/lib \
            --cov-report=xml --cov-report=term-missing \
            --timeout=300
      
      - name: Upload coverage reports
        uses: actions/upload-artifact@v3
        with:
          name: coverage-unit-tests
          path: coverage.xml

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-timeout
      
      - name: Verify anonymized test data
        run: |
          # Ensure anonymized test data is available
          if [ ! -f "${{ env.ANONYMIZED_SMALL_DATA }}" ]; then
            echo "ERROR: Anonymized test data not found!"
            exit 1
          fi
          echo "Using anonymized test data for integration tests"
      
      - name: Run integration tests
        run: |
          export TEST_DATA_PATH="${{ env.ANONYMIZED_SMALL_DATA }}"
          pytest backend/tests/integration/ -v --timeout=300 -m "not gpu"

  gpu-tests:
    name: GPU Integration Tests
    runs-on: [self-hosted, gpu]
    needs: integration-tests
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python with CUDA
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install CUDA toolkit
        run: |
          wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb
          sudo dpkg -i cuda-keyring_1.0-1_all.deb
          sudo apt-get update
          sudo apt-get -y install cuda-toolkit-11-8
      
      - name: Install dependencies with GPU support
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-gpu.txt
          pip install pytest pytest-timeout
      
      - name: Run GPU integration tests
        run: |
          export CUDA_HOME=/usr/local/cuda-11.8
          export PATH=$CUDA_HOME/bin:$PATH
          export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
          pytest backend/tests/gpu/ -v --timeout=600 -m gpu
      
      - name: Run cuDF performance benchmarks
        run: |
          python backend/tests/benchmark_cudf_performance.py
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: gpu-benchmark-results
          path: output/benchmarks/

  regression-tests:
    name: Parquet Pipeline Validation Tests
    runs-on: ubuntu-latest
    needs: integration-tests
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-html
      
      - name: Run all algorithm tests
        run: |
          pytest backend/tests/test_all_algorithms.py -v --timeout=600
      
      - name: Run regression validation tests
        run: |
          pytest backend/tests/integration/test_parquet_cudf_workflow.py::TestDataFormatCompatibility -v \
            --html=validation_report.html --self-contained-html
      
      - name: Upload validation report
        uses: actions/upload-artifact@v3
        with:
          name: validation-report
          path: validation_report.html
        if: always()

  performance-tests:
    name: Performance Validation
    runs-on: ubuntu-latest
    needs: regression-tests
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-benchmark
      
      - name: Run performance benchmarks
        run: |
          pytest backend/tests/performance/test_performance_benchmarks.py -v \
            --benchmark-only --benchmark-autosave
      
      - name: Check performance regression
        run: |
          if [ -f ".benchmarks/Linux-*_*/0001_*.json" ]; then
            python scripts/check_performance_regression.py --results .benchmarks/Linux-*_*/0001_*.json
          else
            echo "No benchmark results found, skipping regression check"
          fi
      
      - name: Upload performance results
        uses: actions/upload-artifact@v3
        with:
          name: performance-results
          path: .benchmarks/

  security-scan:
    name: Security Vulnerability Scan
    runs-on: ubuntu-latest
    needs: lint
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install bandit
        run: |
          python -m pip install --upgrade pip
          pip install bandit[toml]
      
      - name: Run security scan
        run: |
          bandit -r backend/ -ll -f json -o bandit-report.json || true
          bandit -r backend/ -ll
      
      - name: Upload security report
        uses: actions/upload-artifact@v3
        with:
          name: security-report
          path: bandit-report.json
        if: always()

  build-artifacts:
    name: Build and Store Artifacts
    runs-on: ubuntu-latest
    needs: [regression-tests, performance-tests, security-scan]
    if: github.event_name == 'push'
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Create deployment package
        run: |
          mkdir -p dist
          cp -r backend dist/
          cp -r config dist/
          cp requirements*.txt dist/
          cp CLAUDE.md dist/
          tar -czf heavy-optimizer-${{ github.sha }}.tar.gz dist/
      
      - name: Upload deployment package
        uses: actions/upload-artifact@v3
        with:
          name: deployment-package
          path: heavy-optimizer-${{ github.sha }}.tar.gz
          retention-days: 30

  notify-status:
    name: Notify Build Status
    runs-on: ubuntu-latest
    needs: [lint, unit-tests, integration-tests, regression-tests, performance-tests, security-scan]
    if: always()
    steps:
      - name: Check build status
        run: |
          if [[ "${{ needs.lint.result }}" == "failure" || \
                "${{ needs.unit-tests.result }}" == "failure" || \
                "${{ needs.integration-tests.result }}" == "failure" || \
                "${{ needs.regression-tests.result }}" == "failure" || \
                "${{ needs.performance-tests.result }}" == "failure" || \
                "${{ needs.security-scan.result }}" == "failure" ]]; then
            echo "Build failed!"
            exit 1
          else
            echo "Build succeeded!"
          fi
      
      - name: Generate CI Summary
        run: |
          echo "## CI/CD Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Code Quality | ${{ needs.lint.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Unit Tests | ${{ needs.unit-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Integration Tests | ${{ needs.integration-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Regression Tests | ${{ needs.regression-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Performance Tests | ${{ needs.performance-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Security Scan | ${{ needs.security-scan.result }} |" >> $GITHUB_STEP_SUMMARY